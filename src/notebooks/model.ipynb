{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('../datasets/edges/ArtistW_sample.csv', delimiter=';', names=['artist_id_from', 'artist_id_to', 'weight'])\n",
    "\n",
    "# Extract artist IDs and edge weights\n",
    "edges = df[['artist_id_from', 'artist_id_to']].values\n",
    "weights = df['weight'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "# Create a graph data object\n",
    "data = Data(edge_index=edge_index, edge_attr=edge_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class GraphAutoencoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphAutoencoder, self).__init__()\n",
    "        self.encoder = GCNConv(in_channels, hidden_channels)\n",
    "        self.decoder = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.encoder(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def decode(self, z):\n",
    "        adj_pred = torch.sigmoid(torch.matmul(z, z.t()))\n",
    "        return adj_pred\n",
    "\n",
    "    def forward(self, data):\n",
    "        z = self.encode(data.x, data.edge_index)\n",
    "        adj_pred = self.decode(z)\n",
    "        return adj_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Conda\\envs\\spotify-network\\Lib\\site-packages\\torch_geometric\\data\\storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_attr', 'edge_index'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.25000089406967163\n",
      "Epoch 2, Loss: 0.25081339478492737\n",
      "Epoch 3, Loss: 0.25002557039260864\n",
      "Epoch 4, Loss: 0.2502765357494354\n",
      "Epoch 5, Loss: 0.2505008280277252\n",
      "Epoch 6, Loss: 0.25027593970298767\n",
      "Epoch 7, Loss: 0.2500344216823578\n",
      "Epoch 8, Loss: 0.25004228949546814\n",
      "Epoch 9, Loss: 0.25019755959510803\n",
      "Epoch 10, Loss: 0.25025197863578796\n",
      "Epoch 11, Loss: 0.25015485286712646\n",
      "Epoch 12, Loss: 0.25003519654273987\n",
      "Epoch 13, Loss: 0.25001025199890137\n",
      "Epoch 14, Loss: 0.25007548928260803\n",
      "Epoch 15, Loss: 0.2501334547996521\n",
      "Epoch 16, Loss: 0.2501166760921478\n",
      "Epoch 17, Loss: 0.250051349401474\n",
      "Epoch 18, Loss: 0.25000688433647156\n",
      "Epoch 19, Loss: 0.2500184178352356\n",
      "Epoch 20, Loss: 0.25005820393562317\n",
      "Epoch 21, Loss: 0.25007495284080505\n",
      "Epoch 22, Loss: 0.2500515282154083\n",
      "Epoch 23, Loss: 0.25001591444015503\n",
      "Epoch 24, Loss: 0.2500036060810089\n",
      "Epoch 25, Loss: 0.25001999735832214\n",
      "Epoch 26, Loss: 0.25003960728645325\n",
      "Epoch 27, Loss: 0.25003838539123535\n",
      "Epoch 28, Loss: 0.25001904368400574\n",
      "Epoch 29, Loss: 0.2500033676624298\n",
      "Epoch 30, Loss: 0.2500056028366089\n",
      "Epoch 31, Loss: 0.2500183582305908\n",
      "Epoch 32, Loss: 0.25002387166023254\n",
      "Epoch 33, Loss: 0.2500157356262207\n",
      "Epoch 34, Loss: 0.2500040531158447\n",
      "Epoch 35, Loss: 0.2500014305114746\n",
      "Epoch 36, Loss: 0.2500079572200775\n",
      "Epoch 37, Loss: 0.25001347064971924\n",
      "Epoch 38, Loss: 0.25001060962677\n",
      "Epoch 39, Loss: 0.2500031888484955\n",
      "Epoch 40, Loss: 0.24999995529651642\n",
      "Epoch 41, Loss: 0.25000324845314026\n",
      "Epoch 42, Loss: 0.25000715255737305\n",
      "Epoch 43, Loss: 0.25000596046447754\n",
      "Epoch 44, Loss: 0.25000137090682983\n",
      "Epoch 45, Loss: 0.2499990016222\n",
      "Epoch 46, Loss: 0.25000083446502686\n",
      "Epoch 47, Loss: 0.2500031888484955\n",
      "Epoch 48, Loss: 0.25000235438346863\n",
      "Epoch 49, Loss: 0.24999932944774628\n",
      "Epoch 50, Loss: 0.24999788403511047\n",
      "Epoch 51, Loss: 0.24999909102916718\n",
      "Epoch 52, Loss: 0.2500002682209015\n",
      "Epoch 53, Loss: 0.2499992847442627\n",
      "Epoch 54, Loss: 0.24999719858169556\n",
      "Epoch 55, Loss: 0.24999642372131348\n",
      "Epoch 56, Loss: 0.24999713897705078\n",
      "Epoch 57, Loss: 0.2499973475933075\n",
      "Epoch 58, Loss: 0.2499961256980896\n",
      "Epoch 59, Loss: 0.24999463558197021\n",
      "Epoch 60, Loss: 0.2499942183494568\n",
      "Epoch 61, Loss: 0.24999430775642395\n",
      "Epoch 62, Loss: 0.2499937117099762\n",
      "Epoch 63, Loss: 0.24999231100082397\n",
      "Epoch 64, Loss: 0.24999110400676727\n",
      "Epoch 65, Loss: 0.24999050796031952\n",
      "Epoch 66, Loss: 0.249989852309227\n",
      "Epoch 67, Loss: 0.24998851120471954\n",
      "Epoch 68, Loss: 0.24998688697814941\n",
      "Epoch 69, Loss: 0.24998554587364197\n",
      "Epoch 70, Loss: 0.24998430907726288\n",
      "Epoch 71, Loss: 0.24998271465301514\n",
      "Epoch 72, Loss: 0.2499806433916092\n",
      "Epoch 73, Loss: 0.24997848272323608\n",
      "Epoch 74, Loss: 0.24997644126415253\n",
      "Epoch 75, Loss: 0.2499741166830063\n",
      "Epoch 76, Loss: 0.2499713897705078\n",
      "Epoch 77, Loss: 0.2499682903289795\n",
      "Epoch 78, Loss: 0.2499651163816452\n",
      "Epoch 79, Loss: 0.2499617338180542\n",
      "Epoch 80, Loss: 0.24995790421962738\n",
      "Epoch 81, Loss: 0.24995367228984833\n",
      "Epoch 82, Loss: 0.24994909763336182\n",
      "Epoch 83, Loss: 0.24994422495365143\n",
      "Epoch 84, Loss: 0.2499389350414276\n",
      "Epoch 85, Loss: 0.2499331384897232\n",
      "Epoch 86, Loss: 0.24992690980434418\n",
      "Epoch 87, Loss: 0.2499203234910965\n",
      "Epoch 88, Loss: 0.24991324543952942\n",
      "Epoch 89, Loss: 0.24990570545196533\n",
      "Epoch 90, Loss: 0.24989767372608185\n",
      "Epoch 91, Loss: 0.24988926947116852\n",
      "Epoch 92, Loss: 0.24988047778606415\n",
      "Epoch 93, Loss: 0.24987128376960754\n",
      "Epoch 94, Loss: 0.2498617172241211\n",
      "Epoch 95, Loss: 0.24985189735889435\n",
      "Epoch 96, Loss: 0.24984192848205566\n",
      "Epoch 97, Loss: 0.24983176589012146\n",
      "Epoch 98, Loss: 0.2498214989900589\n",
      "Epoch 99, Loss: 0.2498113512992859\n",
      "Epoch 100, Loss: 0.24980133771896362\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "# Crear datos de ejemplo\n",
    "num_nodes = data.num_nodes\n",
    "data.x = torch.eye(num_nodes)  # Usar una matriz identidad como características de los nodos\n",
    "\n",
    "# Inicializar el modelo, el optimizador y la función de pérdida\n",
    "model = GraphAutoencoder(in_channels=num_nodes, hidden_channels=16, out_channels=num_nodes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Convertir el grafo a una matriz de adyacencia densa\n",
    "adj = to_dense_adj(data.edge_index).squeeze()\n",
    "\n",
    "# Bucle de entrenamiento\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    adj_pred = model(data)\n",
    "    loss = criterion(adj_pred, adj)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las representaciones de los nodos\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    node_embeddings = model.encode(data.x, data.edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read labels\n",
    "labels = pd.read_json('../datasets/edges/ArtistW_labels.json', typ='series')\n",
    "# Suponiendo que 'labels' es un tensor con las etiquetas de los nodos\n",
    "labels = torch.tensor(labels.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.107795238494873\n",
      "Epoch 2, Loss: 3.0013351440429688\n",
      "Epoch 3, Loss: 2.903984546661377\n",
      "Epoch 4, Loss: 2.8109779357910156\n",
      "Epoch 5, Loss: 2.722201347351074\n",
      "Epoch 6, Loss: 2.640368938446045\n",
      "Epoch 7, Loss: 2.56477427482605\n",
      "Epoch 8, Loss: 2.499610662460327\n",
      "Epoch 9, Loss: 2.4440035820007324\n",
      "Epoch 10, Loss: 2.397148847579956\n",
      "Epoch 11, Loss: 2.3637447357177734\n",
      "Epoch 12, Loss: 2.3397796154022217\n",
      "Epoch 13, Loss: 2.324061870574951\n",
      "Epoch 14, Loss: 2.3161611557006836\n",
      "Epoch 15, Loss: 2.3156545162200928\n",
      "Epoch 16, Loss: 2.312835693359375\n",
      "Epoch 17, Loss: 2.3098535537719727\n",
      "Epoch 18, Loss: 2.3081417083740234\n",
      "Epoch 19, Loss: 2.3071553707122803\n",
      "Epoch 20, Loss: 2.2936253547668457\n",
      "Epoch 21, Loss: 2.2905325889587402\n",
      "Epoch 22, Loss: 2.2818758487701416\n",
      "Epoch 23, Loss: 2.2727932929992676\n",
      "Epoch 24, Loss: 2.265475034713745\n",
      "Epoch 25, Loss: 2.2605416774749756\n",
      "Epoch 26, Loss: 2.256594181060791\n",
      "Epoch 27, Loss: 2.253465175628662\n",
      "Epoch 28, Loss: 2.25321888923645\n",
      "Epoch 29, Loss: 2.2492289543151855\n",
      "Epoch 30, Loss: 2.250101327896118\n",
      "Epoch 31, Loss: 2.248411178588867\n",
      "Epoch 32, Loss: 2.24975848197937\n",
      "Epoch 33, Loss: 2.24737286567688\n",
      "Epoch 34, Loss: 2.2491917610168457\n",
      "Epoch 35, Loss: 2.2470805644989014\n",
      "Epoch 36, Loss: 2.245922327041626\n",
      "Epoch 37, Loss: 2.245441198348999\n",
      "Epoch 38, Loss: 2.2445244789123535\n",
      "Epoch 39, Loss: 2.2386975288391113\n",
      "Epoch 40, Loss: 2.2406907081604004\n",
      "Epoch 41, Loss: 2.237114191055298\n",
      "Epoch 42, Loss: 2.2375032901763916\n",
      "Epoch 43, Loss: 2.237914562225342\n",
      "Epoch 44, Loss: 2.23926043510437\n",
      "Epoch 45, Loss: 2.2361679077148438\n",
      "Epoch 46, Loss: 2.2369771003723145\n",
      "Epoch 47, Loss: 2.236421585083008\n",
      "Epoch 48, Loss: 2.236966848373413\n",
      "Epoch 49, Loss: 2.235964298248291\n",
      "Epoch 50, Loss: 2.2355613708496094\n",
      "Epoch 51, Loss: 2.236480236053467\n",
      "Epoch 52, Loss: 2.2355637550354004\n",
      "Epoch 53, Loss: 2.231741428375244\n",
      "Epoch 54, Loss: 2.233985662460327\n",
      "Epoch 55, Loss: 2.2317426204681396\n",
      "Epoch 56, Loss: 2.2313506603240967\n",
      "Epoch 57, Loss: 2.231078624725342\n",
      "Epoch 58, Loss: 2.2312657833099365\n",
      "Epoch 59, Loss: 2.2306411266326904\n",
      "Epoch 60, Loss: 2.2330985069274902\n",
      "Epoch 61, Loss: 2.2317934036254883\n",
      "Epoch 62, Loss: 2.2293388843536377\n",
      "Epoch 63, Loss: 2.2294487953186035\n",
      "Epoch 64, Loss: 2.230363607406616\n",
      "Epoch 65, Loss: 2.229646682739258\n",
      "Epoch 66, Loss: 2.2297046184539795\n",
      "Epoch 67, Loss: 2.227745294570923\n",
      "Epoch 68, Loss: 2.2292916774749756\n",
      "Epoch 69, Loss: 2.2273552417755127\n",
      "Epoch 70, Loss: 2.2291698455810547\n",
      "Epoch 71, Loss: 2.2279345989227295\n",
      "Epoch 72, Loss: 2.227330446243286\n",
      "Epoch 73, Loss: 2.227186441421509\n",
      "Epoch 74, Loss: 2.2276737689971924\n",
      "Epoch 75, Loss: 2.225928783416748\n",
      "Epoch 76, Loss: 2.226793050765991\n",
      "Epoch 77, Loss: 2.226469039916992\n",
      "Epoch 78, Loss: 2.2251343727111816\n",
      "Epoch 79, Loss: 2.2238574028015137\n",
      "Epoch 80, Loss: 2.2245945930480957\n",
      "Epoch 81, Loss: 2.224553346633911\n",
      "Epoch 82, Loss: 2.223132848739624\n",
      "Epoch 83, Loss: 2.224093437194824\n",
      "Epoch 84, Loss: 2.2221689224243164\n",
      "Epoch 85, Loss: 2.2217366695404053\n",
      "Epoch 86, Loss: 2.2231369018554688\n",
      "Epoch 87, Loss: 2.2212564945220947\n",
      "Epoch 88, Loss: 2.2230947017669678\n",
      "Epoch 89, Loss: 2.2241406440734863\n",
      "Epoch 90, Loss: 2.2225191593170166\n",
      "Epoch 91, Loss: 2.2230191230773926\n",
      "Epoch 92, Loss: 2.221853733062744\n",
      "Epoch 93, Loss: 2.2212934494018555\n",
      "Epoch 94, Loss: 2.2224960327148438\n",
      "Epoch 95, Loss: 2.219276189804077\n",
      "Epoch 96, Loss: 2.2206647396087646\n",
      "Epoch 97, Loss: 2.2197835445404053\n",
      "Epoch 98, Loss: 2.2203855514526367\n",
      "Epoch 99, Loss: 2.220975637435913\n",
      "Epoch 100, Loss: 2.2194292545318604\n",
      "Epoch 101, Loss: 2.2202565670013428\n",
      "Epoch 102, Loss: 2.2172534465789795\n",
      "Epoch 103, Loss: 2.219748020172119\n",
      "Epoch 104, Loss: 2.221201181411743\n",
      "Epoch 105, Loss: 2.2199172973632812\n",
      "Epoch 106, Loss: 2.216156005859375\n",
      "Epoch 107, Loss: 2.218623638153076\n",
      "Epoch 108, Loss: 2.21758770942688\n",
      "Epoch 109, Loss: 2.219143867492676\n",
      "Epoch 110, Loss: 2.2184126377105713\n",
      "Epoch 111, Loss: 2.2171859741210938\n",
      "Epoch 112, Loss: 2.216836929321289\n",
      "Epoch 113, Loss: 2.218813896179199\n",
      "Epoch 114, Loss: 2.2172648906707764\n",
      "Epoch 115, Loss: 2.2157340049743652\n",
      "Epoch 116, Loss: 2.2171785831451416\n",
      "Epoch 117, Loss: 2.215838670730591\n",
      "Epoch 118, Loss: 2.21647047996521\n",
      "Epoch 119, Loss: 2.2153666019439697\n",
      "Epoch 120, Loss: 2.2168424129486084\n",
      "Epoch 121, Loss: 2.214263439178467\n",
      "Epoch 122, Loss: 2.215155839920044\n",
      "Epoch 123, Loss: 2.2164881229400635\n",
      "Epoch 124, Loss: 2.214529037475586\n",
      "Epoch 125, Loss: 2.214080810546875\n",
      "Epoch 126, Loss: 2.2156922817230225\n",
      "Epoch 127, Loss: 2.214280843734741\n",
      "Epoch 128, Loss: 2.2139246463775635\n",
      "Epoch 129, Loss: 2.212554931640625\n",
      "Epoch 130, Loss: 2.2150282859802246\n",
      "Epoch 131, Loss: 2.2161622047424316\n",
      "Epoch 132, Loss: 2.2131357192993164\n",
      "Epoch 133, Loss: 2.2151920795440674\n",
      "Epoch 134, Loss: 2.2147319316864014\n",
      "Epoch 135, Loss: 2.2135250568389893\n",
      "Epoch 136, Loss: 2.2124183177948\n",
      "Epoch 137, Loss: 2.2152981758117676\n",
      "Epoch 138, Loss: 2.2121288776397705\n",
      "Epoch 139, Loss: 2.2122881412506104\n",
      "Epoch 140, Loss: 2.2134287357330322\n",
      "Epoch 141, Loss: 2.2142820358276367\n",
      "Epoch 142, Loss: 2.2139434814453125\n",
      "Epoch 143, Loss: 2.2127227783203125\n",
      "Epoch 144, Loss: 2.21209979057312\n",
      "Epoch 145, Loss: 2.2098777294158936\n",
      "Epoch 146, Loss: 2.211303472518921\n",
      "Epoch 147, Loss: 2.2121357917785645\n",
      "Epoch 148, Loss: 2.212973117828369\n",
      "Epoch 149, Loss: 2.2122535705566406\n",
      "Epoch 150, Loss: 2.2108895778656006\n",
      "Epoch 151, Loss: 2.2124440670013428\n",
      "Epoch 152, Loss: 2.2092502117156982\n",
      "Epoch 153, Loss: 2.212908983230591\n",
      "Epoch 154, Loss: 2.212407350540161\n",
      "Epoch 155, Loss: 2.211850643157959\n",
      "Epoch 156, Loss: 2.211871385574341\n",
      "Epoch 157, Loss: 2.2123823165893555\n",
      "Epoch 158, Loss: 2.2116472721099854\n",
      "Epoch 159, Loss: 2.2091500759124756\n",
      "Epoch 160, Loss: 2.2117323875427246\n",
      "Epoch 161, Loss: 2.2112085819244385\n",
      "Epoch 162, Loss: 2.2112162113189697\n",
      "Epoch 163, Loss: 2.2101266384124756\n",
      "Epoch 164, Loss: 2.2099688053131104\n",
      "Epoch 165, Loss: 2.2100913524627686\n",
      "Epoch 166, Loss: 2.208571672439575\n",
      "Epoch 167, Loss: 2.2096686363220215\n",
      "Epoch 168, Loss: 2.2090959548950195\n",
      "Epoch 169, Loss: 2.2100749015808105\n",
      "Epoch 170, Loss: 2.2094740867614746\n",
      "Epoch 171, Loss: 2.2092151641845703\n",
      "Epoch 172, Loss: 2.2102818489074707\n",
      "Epoch 173, Loss: 2.2089223861694336\n",
      "Epoch 174, Loss: 2.209365129470825\n",
      "Epoch 175, Loss: 2.210827589035034\n",
      "Epoch 176, Loss: 2.208423137664795\n",
      "Epoch 177, Loss: 2.210157871246338\n",
      "Epoch 178, Loss: 2.2109577655792236\n",
      "Epoch 179, Loss: 2.2092819213867188\n",
      "Epoch 180, Loss: 2.209463357925415\n",
      "Epoch 181, Loss: 2.208117961883545\n",
      "Epoch 182, Loss: 2.2092790603637695\n",
      "Epoch 183, Loss: 2.2099316120147705\n",
      "Epoch 184, Loss: 2.209207773208618\n",
      "Epoch 185, Loss: 2.208944797515869\n",
      "Epoch 186, Loss: 2.208930730819702\n",
      "Epoch 187, Loss: 2.207603931427002\n",
      "Epoch 188, Loss: 2.2083380222320557\n",
      "Epoch 189, Loss: 2.209299325942993\n",
      "Epoch 190, Loss: 2.207716941833496\n",
      "Epoch 191, Loss: 2.2079479694366455\n",
      "Epoch 192, Loss: 2.207609176635742\n",
      "Epoch 193, Loss: 2.207099199295044\n",
      "Epoch 194, Loss: 2.2070627212524414\n",
      "Epoch 195, Loss: 2.209329843521118\n",
      "Epoch 196, Loss: 2.208109140396118\n",
      "Epoch 197, Loss: 2.208449602127075\n",
      "Epoch 198, Loss: 2.2073891162872314\n",
      "Epoch 199, Loss: 2.2070319652557373\n",
      "Epoch 200, Loss: 2.2067677974700928\n",
      "Epoch 201, Loss: 2.2102599143981934\n",
      "Epoch 202, Loss: 2.2056493759155273\n",
      "Epoch 203, Loss: 2.2067737579345703\n",
      "Epoch 204, Loss: 2.207179069519043\n",
      "Epoch 205, Loss: 2.2087574005126953\n",
      "Epoch 206, Loss: 2.2075352668762207\n",
      "Epoch 207, Loss: 2.2075881958007812\n",
      "Epoch 208, Loss: 2.207437038421631\n",
      "Epoch 209, Loss: 2.2059197425842285\n",
      "Epoch 210, Loss: 2.2058041095733643\n",
      "Epoch 211, Loss: 2.2067816257476807\n",
      "Epoch 212, Loss: 2.206743001937866\n",
      "Epoch 213, Loss: 2.2081098556518555\n",
      "Epoch 214, Loss: 2.206911325454712\n",
      "Epoch 215, Loss: 2.2061524391174316\n",
      "Epoch 216, Loss: 2.2077248096466064\n",
      "Epoch 217, Loss: 2.2080469131469727\n",
      "Epoch 218, Loss: 2.2066760063171387\n",
      "Epoch 219, Loss: 2.208134412765503\n",
      "Epoch 220, Loss: 2.2048001289367676\n",
      "Epoch 221, Loss: 2.206230640411377\n",
      "Epoch 222, Loss: 2.2075533866882324\n",
      "Epoch 223, Loss: 2.2079927921295166\n",
      "Epoch 224, Loss: 2.207040548324585\n",
      "Epoch 225, Loss: 2.206598997116089\n",
      "Epoch 226, Loss: 2.20635724067688\n",
      "Epoch 227, Loss: 2.2052595615386963\n",
      "Epoch 228, Loss: 2.205747127532959\n",
      "Epoch 229, Loss: 2.2056469917297363\n",
      "Epoch 230, Loss: 2.2071099281311035\n",
      "Epoch 231, Loss: 2.206941604614258\n",
      "Epoch 232, Loss: 2.207239866256714\n",
      "Epoch 233, Loss: 2.2066688537597656\n",
      "Epoch 234, Loss: 2.2065629959106445\n",
      "Epoch 235, Loss: 2.2046244144439697\n",
      "Epoch 236, Loss: 2.2065165042877197\n",
      "Epoch 237, Loss: 2.2069051265716553\n",
      "Epoch 238, Loss: 2.207371473312378\n",
      "Epoch 239, Loss: 2.2069051265716553\n",
      "Epoch 240, Loss: 2.204275369644165\n",
      "Epoch 241, Loss: 2.205765962600708\n",
      "Epoch 242, Loss: 2.205451488494873\n",
      "Epoch 243, Loss: 2.206488609313965\n",
      "Epoch 244, Loss: 2.2052807807922363\n",
      "Epoch 245, Loss: 2.205548048019409\n",
      "Epoch 246, Loss: 2.2064483165740967\n",
      "Epoch 247, Loss: 2.2047488689422607\n",
      "Epoch 248, Loss: 2.2056849002838135\n",
      "Epoch 249, Loss: 2.205145835876465\n",
      "Epoch 250, Loss: 2.2034058570861816\n",
      "Epoch 251, Loss: 2.2060861587524414\n",
      "Epoch 252, Loss: 2.205611228942871\n",
      "Epoch 253, Loss: 2.205024003982544\n",
      "Epoch 254, Loss: 2.205686569213867\n",
      "Epoch 255, Loss: 2.20650053024292\n",
      "Epoch 256, Loss: 2.205085277557373\n",
      "Epoch 257, Loss: 2.2065422534942627\n",
      "Epoch 258, Loss: 2.2056643962860107\n",
      "Epoch 259, Loss: 2.205218553543091\n",
      "Epoch 260, Loss: 2.2062907218933105\n",
      "Epoch 261, Loss: 2.2045555114746094\n",
      "Epoch 262, Loss: 2.2048208713531494\n",
      "Epoch 263, Loss: 2.2066919803619385\n",
      "Epoch 264, Loss: 2.205296754837036\n",
      "Epoch 265, Loss: 2.204726219177246\n",
      "Epoch 266, Loss: 2.2046048641204834\n",
      "Epoch 267, Loss: 2.205962896347046\n",
      "Epoch 268, Loss: 2.204148292541504\n",
      "Epoch 269, Loss: 2.2044382095336914\n",
      "Epoch 270, Loss: 2.204679489135742\n",
      "Epoch 271, Loss: 2.2059266567230225\n",
      "Epoch 272, Loss: 2.204720973968506\n",
      "Epoch 273, Loss: 2.2051544189453125\n",
      "Epoch 274, Loss: 2.204482078552246\n",
      "Epoch 275, Loss: 2.2039759159088135\n",
      "Epoch 276, Loss: 2.2039661407470703\n",
      "Epoch 277, Loss: 2.205657958984375\n",
      "Epoch 278, Loss: 2.204525947570801\n",
      "Epoch 279, Loss: 2.2060868740081787\n",
      "Epoch 280, Loss: 2.204984188079834\n",
      "Epoch 281, Loss: 2.20613694190979\n",
      "Epoch 282, Loss: 2.2051310539245605\n",
      "Epoch 283, Loss: 2.2052972316741943\n",
      "Epoch 284, Loss: 2.2058513164520264\n",
      "Epoch 285, Loss: 2.2062859535217285\n",
      "Epoch 286, Loss: 2.2035837173461914\n",
      "Epoch 287, Loss: 2.2057735919952393\n",
      "Epoch 288, Loss: 2.2046797275543213\n",
      "Epoch 289, Loss: 2.2038307189941406\n",
      "Epoch 290, Loss: 2.2054851055145264\n",
      "Epoch 291, Loss: 2.206597089767456\n",
      "Epoch 292, Loss: 2.2040205001831055\n",
      "Epoch 293, Loss: 2.2061657905578613\n",
      "Epoch 294, Loss: 2.203550100326538\n",
      "Epoch 295, Loss: 2.2059619426727295\n",
      "Epoch 296, Loss: 2.2040622234344482\n",
      "Epoch 297, Loss: 2.2058050632476807\n",
      "Epoch 298, Loss: 2.2061872482299805\n",
      "Epoch 299, Loss: 2.205503225326538\n",
      "Epoch 300, Loss: 2.205408811569214\n",
      "Epoch 301, Loss: 2.2043089866638184\n",
      "Epoch 302, Loss: 2.2039802074432373\n",
      "Epoch 303, Loss: 2.2046689987182617\n",
      "Epoch 304, Loss: 2.2057714462280273\n",
      "Epoch 305, Loss: 2.20485782623291\n",
      "Epoch 306, Loss: 2.2052812576293945\n",
      "Epoch 307, Loss: 2.204511880874634\n",
      "Epoch 308, Loss: 2.2059552669525146\n",
      "Epoch 309, Loss: 2.203787088394165\n",
      "Epoch 310, Loss: 2.2047040462493896\n",
      "Epoch 311, Loss: 2.2038652896881104\n",
      "Epoch 312, Loss: 2.204677104949951\n",
      "Epoch 313, Loss: 2.2029855251312256\n",
      "Epoch 314, Loss: 2.2033236026763916\n",
      "Epoch 315, Loss: 2.2039718627929688\n",
      "Epoch 316, Loss: 2.20343017578125\n",
      "Epoch 317, Loss: 2.2035555839538574\n",
      "Epoch 318, Loss: 2.2046432495117188\n",
      "Epoch 319, Loss: 2.203268051147461\n",
      "Epoch 320, Loss: 2.2047712802886963\n",
      "Epoch 321, Loss: 2.2030229568481445\n",
      "Epoch 322, Loss: 2.205225706100464\n",
      "Epoch 323, Loss: 2.204256534576416\n",
      "Epoch 324, Loss: 2.202930450439453\n",
      "Epoch 325, Loss: 2.20393443107605\n",
      "Epoch 326, Loss: 2.20462965965271\n",
      "Epoch 327, Loss: 2.204460620880127\n",
      "Epoch 328, Loss: 2.205049514770508\n",
      "Epoch 329, Loss: 2.2031548023223877\n",
      "Epoch 330, Loss: 2.203824043273926\n",
      "Epoch 331, Loss: 2.2050747871398926\n",
      "Epoch 332, Loss: 2.2039294242858887\n",
      "Epoch 333, Loss: 2.203824996948242\n",
      "Epoch 334, Loss: 2.202876329421997\n",
      "Epoch 335, Loss: 2.20379376411438\n",
      "Epoch 336, Loss: 2.203721761703491\n",
      "Epoch 337, Loss: 2.203777551651001\n",
      "Epoch 338, Loss: 2.204629421234131\n",
      "Epoch 339, Loss: 2.203078031539917\n",
      "Epoch 340, Loss: 2.2068567276000977\n",
      "Epoch 341, Loss: 2.2034242153167725\n",
      "Epoch 342, Loss: 2.2034497261047363\n",
      "Epoch 343, Loss: 2.205507755279541\n",
      "Epoch 344, Loss: 2.2029762268066406\n",
      "Epoch 345, Loss: 2.204516649246216\n",
      "Epoch 346, Loss: 2.2049734592437744\n",
      "Epoch 347, Loss: 2.2039501667022705\n",
      "Epoch 348, Loss: 2.2032835483551025\n",
      "Epoch 349, Loss: 2.2048988342285156\n",
      "Epoch 350, Loss: 2.204908847808838\n",
      "Epoch 351, Loss: 2.2036454677581787\n",
      "Epoch 352, Loss: 2.2030532360076904\n",
      "Epoch 353, Loss: 2.2031285762786865\n",
      "Epoch 354, Loss: 2.2039294242858887\n",
      "Epoch 355, Loss: 2.2034289836883545\n",
      "Epoch 356, Loss: 2.2035000324249268\n",
      "Epoch 357, Loss: 2.2036705017089844\n",
      "Epoch 358, Loss: 2.2047228813171387\n",
      "Epoch 359, Loss: 2.20352840423584\n",
      "Epoch 360, Loss: 2.204314708709717\n",
      "Epoch 361, Loss: 2.205498218536377\n",
      "Epoch 362, Loss: 2.2037956714630127\n",
      "Epoch 363, Loss: 2.205331563949585\n",
      "Epoch 364, Loss: 2.202857732772827\n",
      "Epoch 365, Loss: 2.203220844268799\n",
      "Epoch 366, Loss: 2.204268217086792\n",
      "Epoch 367, Loss: 2.202583074569702\n",
      "Epoch 368, Loss: 2.203284502029419\n",
      "Epoch 369, Loss: 2.2041547298431396\n",
      "Epoch 370, Loss: 2.2057952880859375\n",
      "Epoch 371, Loss: 2.203636646270752\n",
      "Epoch 372, Loss: 2.204033374786377\n",
      "Epoch 373, Loss: 2.2036361694335938\n",
      "Epoch 374, Loss: 2.2025485038757324\n",
      "Epoch 375, Loss: 2.201859951019287\n",
      "Epoch 376, Loss: 2.2054126262664795\n",
      "Epoch 377, Loss: 2.2033767700195312\n",
      "Epoch 378, Loss: 2.20499324798584\n",
      "Epoch 379, Loss: 2.204084873199463\n",
      "Epoch 380, Loss: 2.203690528869629\n",
      "Epoch 381, Loss: 2.2033729553222656\n",
      "Epoch 382, Loss: 2.2040021419525146\n",
      "Epoch 383, Loss: 2.203449010848999\n",
      "Epoch 384, Loss: 2.2049660682678223\n",
      "Epoch 385, Loss: 2.203657627105713\n",
      "Epoch 386, Loss: 2.2038919925689697\n",
      "Epoch 387, Loss: 2.205439805984497\n",
      "Epoch 388, Loss: 2.2031917572021484\n",
      "Epoch 389, Loss: 2.2053627967834473\n",
      "Epoch 390, Loss: 2.2027156352996826\n",
      "Epoch 391, Loss: 2.2039732933044434\n",
      "Epoch 392, Loss: 2.2046492099761963\n",
      "Epoch 393, Loss: 2.204582691192627\n",
      "Epoch 394, Loss: 2.202942371368408\n",
      "Epoch 395, Loss: 2.2033464908599854\n",
      "Epoch 396, Loss: 2.203793525695801\n",
      "Epoch 397, Loss: 2.203963041305542\n",
      "Epoch 398, Loss: 2.202863931655884\n",
      "Epoch 399, Loss: 2.2045581340789795\n",
      "Epoch 400, Loss: 2.2038509845733643\n",
      "Epoch 401, Loss: 2.2043747901916504\n",
      "Epoch 402, Loss: 2.204070806503296\n",
      "Epoch 403, Loss: 2.203248977661133\n",
      "Epoch 404, Loss: 2.2042930126190186\n",
      "Epoch 405, Loss: 2.2047598361968994\n",
      "Epoch 406, Loss: 2.203998327255249\n",
      "Epoch 407, Loss: 2.203465700149536\n",
      "Epoch 408, Loss: 2.2024502754211426\n",
      "Epoch 409, Loss: 2.2021100521087646\n",
      "Epoch 410, Loss: 2.20355224609375\n",
      "Epoch 411, Loss: 2.2030060291290283\n",
      "Epoch 412, Loss: 2.2053515911102295\n",
      "Epoch 413, Loss: 2.202343702316284\n",
      "Epoch 414, Loss: 2.202237367630005\n",
      "Epoch 415, Loss: 2.2024593353271484\n",
      "Epoch 416, Loss: 2.204979181289673\n",
      "Epoch 417, Loss: 2.2051382064819336\n",
      "Epoch 418, Loss: 2.2020509243011475\n",
      "Epoch 419, Loss: 2.2031311988830566\n",
      "Epoch 420, Loss: 2.201908588409424\n",
      "Epoch 421, Loss: 2.2020280361175537\n",
      "Epoch 422, Loss: 2.2023375034332275\n",
      "Epoch 423, Loss: 2.202101469039917\n",
      "Epoch 424, Loss: 2.2024834156036377\n",
      "Epoch 425, Loss: 2.202176094055176\n",
      "Epoch 426, Loss: 2.203963279724121\n",
      "Epoch 427, Loss: 2.203995943069458\n",
      "Epoch 428, Loss: 2.203726291656494\n",
      "Epoch 429, Loss: 2.2044947147369385\n",
      "Epoch 430, Loss: 2.2037353515625\n",
      "Epoch 431, Loss: 2.2037811279296875\n",
      "Epoch 432, Loss: 2.2047886848449707\n",
      "Epoch 433, Loss: 2.2034826278686523\n",
      "Epoch 434, Loss: 2.203153371810913\n",
      "Epoch 435, Loss: 2.2047595977783203\n",
      "Epoch 436, Loss: 2.2039756774902344\n",
      "Epoch 437, Loss: 2.20169734954834\n",
      "Epoch 438, Loss: 2.2045845985412598\n",
      "Epoch 439, Loss: 2.2033944129943848\n",
      "Epoch 440, Loss: 2.2040371894836426\n",
      "Epoch 441, Loss: 2.2023861408233643\n",
      "Epoch 442, Loss: 2.2048659324645996\n",
      "Epoch 443, Loss: 2.2017765045166016\n",
      "Epoch 444, Loss: 2.2034080028533936\n",
      "Epoch 445, Loss: 2.2033255100250244\n",
      "Epoch 446, Loss: 2.203171968460083\n",
      "Epoch 447, Loss: 2.202282428741455\n",
      "Epoch 448, Loss: 2.2033133506774902\n",
      "Epoch 449, Loss: 2.203860282897949\n",
      "Epoch 450, Loss: 2.2020978927612305\n",
      "Epoch 451, Loss: 2.203571081161499\n",
      "Epoch 452, Loss: 2.200371742248535\n",
      "Epoch 453, Loss: 2.2017645835876465\n",
      "Epoch 454, Loss: 2.2054495811462402\n",
      "Epoch 455, Loss: 2.203857660293579\n",
      "Epoch 456, Loss: 2.203016996383667\n",
      "Epoch 457, Loss: 2.201880693435669\n",
      "Epoch 458, Loss: 2.203610420227051\n",
      "Epoch 459, Loss: 2.2020418643951416\n",
      "Epoch 460, Loss: 2.2029778957366943\n",
      "Epoch 461, Loss: 2.2030327320098877\n",
      "Epoch 462, Loss: 2.203788995742798\n",
      "Epoch 463, Loss: 2.2034239768981934\n",
      "Epoch 464, Loss: 2.2037322521209717\n",
      "Epoch 465, Loss: 2.202486038208008\n",
      "Epoch 466, Loss: 2.2034475803375244\n",
      "Epoch 467, Loss: 2.2032082080841064\n",
      "Epoch 468, Loss: 2.2026026248931885\n",
      "Epoch 469, Loss: 2.2044191360473633\n",
      "Epoch 470, Loss: 2.202573299407959\n",
      "Epoch 471, Loss: 2.202606439590454\n",
      "Epoch 472, Loss: 2.202493906021118\n",
      "Epoch 473, Loss: 2.203490972518921\n",
      "Epoch 474, Loss: 2.2035393714904785\n",
      "Epoch 475, Loss: 2.203090190887451\n",
      "Epoch 476, Loss: 2.203207015991211\n",
      "Epoch 477, Loss: 2.2034642696380615\n",
      "Epoch 478, Loss: 2.2034034729003906\n",
      "Epoch 479, Loss: 2.2036800384521484\n",
      "Epoch 480, Loss: 2.2021677494049072\n",
      "Epoch 481, Loss: 2.202204942703247\n",
      "Epoch 482, Loss: 2.202158212661743\n",
      "Epoch 483, Loss: 2.201957941055298\n",
      "Epoch 484, Loss: 2.203272819519043\n",
      "Epoch 485, Loss: 2.2027266025543213\n",
      "Epoch 486, Loss: 2.2033185958862305\n",
      "Epoch 487, Loss: 2.203077554702759\n",
      "Epoch 488, Loss: 2.2024903297424316\n",
      "Epoch 489, Loss: 2.2030210494995117\n",
      "Epoch 490, Loss: 2.202087640762329\n",
      "Epoch 491, Loss: 2.2024765014648438\n",
      "Epoch 492, Loss: 2.203094005584717\n",
      "Epoch 493, Loss: 2.2030189037323\n",
      "Epoch 494, Loss: 2.203382730484009\n",
      "Epoch 495, Loss: 2.203282117843628\n",
      "Epoch 496, Loss: 2.202608346939087\n",
      "Epoch 497, Loss: 2.202664375305176\n",
      "Epoch 498, Loss: 2.2021450996398926\n",
      "Epoch 499, Loss: 2.20202374458313\n",
      "Epoch 500, Loss: 2.203287363052368\n",
      "Epoch 501, Loss: 2.2016937732696533\n",
      "Epoch 502, Loss: 2.2025132179260254\n",
      "Epoch 503, Loss: 2.2016611099243164\n",
      "Epoch 504, Loss: 2.2013208866119385\n",
      "Epoch 505, Loss: 2.2017531394958496\n",
      "Epoch 506, Loss: 2.201216220855713\n",
      "Epoch 507, Loss: 2.203582286834717\n",
      "Epoch 508, Loss: 2.2029988765716553\n",
      "Epoch 509, Loss: 2.202745199203491\n",
      "Epoch 510, Loss: 2.2017838954925537\n",
      "Epoch 511, Loss: 2.204395055770874\n",
      "Epoch 512, Loss: 2.20198392868042\n",
      "Epoch 513, Loss: 2.203575372695923\n",
      "Epoch 514, Loss: 2.2036094665527344\n",
      "Epoch 515, Loss: 2.199429988861084\n",
      "Epoch 516, Loss: 2.20127534866333\n",
      "Epoch 517, Loss: 2.2006497383117676\n",
      "Epoch 518, Loss: 2.2017152309417725\n",
      "Epoch 519, Loss: 2.201354503631592\n",
      "Epoch 520, Loss: 2.202033758163452\n",
      "Epoch 521, Loss: 2.2019476890563965\n",
      "Epoch 522, Loss: 2.199618339538574\n",
      "Epoch 523, Loss: 2.2018537521362305\n",
      "Epoch 524, Loss: 2.2036359310150146\n",
      "Epoch 525, Loss: 2.2022969722747803\n",
      "Epoch 526, Loss: 2.2034215927124023\n",
      "Epoch 527, Loss: 2.2026290893554688\n",
      "Epoch 528, Loss: 2.2024455070495605\n",
      "Epoch 529, Loss: 2.201824188232422\n",
      "Epoch 530, Loss: 2.202561616897583\n",
      "Epoch 531, Loss: 2.2013297080993652\n",
      "Epoch 532, Loss: 2.204624891281128\n",
      "Epoch 533, Loss: 2.202380657196045\n",
      "Epoch 534, Loss: 2.202014207839966\n",
      "Epoch 535, Loss: 2.2022476196289062\n",
      "Epoch 536, Loss: 2.201000213623047\n",
      "Epoch 537, Loss: 2.201270818710327\n",
      "Epoch 538, Loss: 2.2031636238098145\n",
      "Epoch 539, Loss: 2.201396942138672\n",
      "Epoch 540, Loss: 2.202120304107666\n",
      "Epoch 541, Loss: 2.1999857425689697\n",
      "Epoch 542, Loss: 2.2012507915496826\n",
      "Epoch 543, Loss: 2.201260566711426\n",
      "Epoch 544, Loss: 2.20257306098938\n",
      "Epoch 545, Loss: 2.20180344581604\n",
      "Epoch 546, Loss: 2.1993560791015625\n",
      "Epoch 547, Loss: 2.201508045196533\n",
      "Epoch 548, Loss: 2.2003118991851807\n",
      "Epoch 549, Loss: 2.1997768878936768\n",
      "Epoch 550, Loss: 2.2012569904327393\n",
      "Epoch 551, Loss: 2.202160358428955\n",
      "Epoch 552, Loss: 2.201216697692871\n",
      "Epoch 553, Loss: 2.2001240253448486\n",
      "Epoch 554, Loss: 2.201385021209717\n",
      "Epoch 555, Loss: 2.202141761779785\n",
      "Epoch 556, Loss: 2.200348138809204\n",
      "Epoch 557, Loss: 2.2017784118652344\n",
      "Epoch 558, Loss: 2.201693534851074\n",
      "Epoch 559, Loss: 2.201720714569092\n",
      "Epoch 560, Loss: 2.2002053260803223\n",
      "Epoch 561, Loss: 2.200212240219116\n",
      "Epoch 562, Loss: 2.2017898559570312\n",
      "Epoch 563, Loss: 2.201122760772705\n",
      "Epoch 564, Loss: 2.2017412185668945\n",
      "Epoch 565, Loss: 2.2007834911346436\n",
      "Epoch 566, Loss: 2.200913429260254\n",
      "Epoch 567, Loss: 2.2018942832946777\n",
      "Epoch 568, Loss: 2.2002336978912354\n",
      "Epoch 569, Loss: 2.200277328491211\n",
      "Epoch 570, Loss: 2.1994729042053223\n",
      "Epoch 571, Loss: 2.2016079425811768\n",
      "Epoch 572, Loss: 2.200355291366577\n",
      "Epoch 573, Loss: 2.1993720531463623\n",
      "Epoch 574, Loss: 2.1981635093688965\n",
      "Epoch 575, Loss: 2.2000460624694824\n",
      "Epoch 576, Loss: 2.2005529403686523\n",
      "Epoch 577, Loss: 2.1986007690429688\n",
      "Epoch 578, Loss: 2.2006702423095703\n",
      "Epoch 579, Loss: 2.2009596824645996\n",
      "Epoch 580, Loss: 2.2002792358398438\n",
      "Epoch 581, Loss: 2.1992900371551514\n",
      "Epoch 582, Loss: 2.199598789215088\n",
      "Epoch 583, Loss: 2.20072340965271\n",
      "Epoch 584, Loss: 2.199532985687256\n",
      "Epoch 585, Loss: 2.2014975547790527\n",
      "Epoch 586, Loss: 2.2006876468658447\n",
      "Epoch 587, Loss: 2.20034122467041\n",
      "Epoch 588, Loss: 2.199738025665283\n",
      "Epoch 589, Loss: 2.200676918029785\n",
      "Epoch 590, Loss: 2.2012219429016113\n",
      "Epoch 591, Loss: 2.2001755237579346\n",
      "Epoch 592, Loss: 2.1998496055603027\n",
      "Epoch 593, Loss: 2.200200319290161\n",
      "Epoch 594, Loss: 2.200078010559082\n",
      "Epoch 595, Loss: 2.1988701820373535\n",
      "Epoch 596, Loss: 2.1988306045532227\n",
      "Epoch 597, Loss: 2.199963092803955\n",
      "Epoch 598, Loss: 2.1981823444366455\n",
      "Epoch 599, Loss: 2.200289011001587\n",
      "Epoch 600, Loss: 2.1986777782440186\n",
      "Epoch 601, Loss: 2.1999099254608154\n",
      "Epoch 602, Loss: 2.1989383697509766\n",
      "Epoch 603, Loss: 2.1997692584991455\n",
      "Epoch 604, Loss: 2.198943614959717\n",
      "Epoch 605, Loss: 2.1990528106689453\n",
      "Epoch 606, Loss: 2.2007734775543213\n",
      "Epoch 607, Loss: 2.19861102104187\n",
      "Epoch 608, Loss: 2.199892520904541\n",
      "Epoch 609, Loss: 2.1984803676605225\n",
      "Epoch 610, Loss: 2.200343370437622\n",
      "Epoch 611, Loss: 2.1976735591888428\n",
      "Epoch 612, Loss: 2.198976993560791\n",
      "Epoch 613, Loss: 2.1998960971832275\n",
      "Epoch 614, Loss: 2.197509765625\n",
      "Epoch 615, Loss: 2.199343681335449\n",
      "Epoch 616, Loss: 2.1986143589019775\n",
      "Epoch 617, Loss: 2.198526620864868\n",
      "Epoch 618, Loss: 2.1969916820526123\n",
      "Epoch 619, Loss: 2.1990818977355957\n",
      "Epoch 620, Loss: 2.1986231803894043\n",
      "Epoch 621, Loss: 2.1980671882629395\n",
      "Epoch 622, Loss: 2.1963179111480713\n",
      "Epoch 623, Loss: 2.1972217559814453\n",
      "Epoch 624, Loss: 2.1980836391448975\n",
      "Epoch 625, Loss: 2.198376417160034\n",
      "Epoch 626, Loss: 2.198233127593994\n",
      "Epoch 627, Loss: 2.1982970237731934\n",
      "Epoch 628, Loss: 2.198678970336914\n",
      "Epoch 629, Loss: 2.197834014892578\n",
      "Epoch 630, Loss: 2.198148012161255\n",
      "Epoch 631, Loss: 2.196558713912964\n",
      "Epoch 632, Loss: 2.19608473777771\n",
      "Epoch 633, Loss: 2.196760654449463\n",
      "Epoch 634, Loss: 2.1987931728363037\n",
      "Epoch 635, Loss: 2.1975455284118652\n",
      "Epoch 636, Loss: 2.196220874786377\n",
      "Epoch 637, Loss: 2.19655704498291\n",
      "Epoch 638, Loss: 2.197051525115967\n",
      "Epoch 639, Loss: 2.19580078125\n",
      "Epoch 640, Loss: 2.195639133453369\n",
      "Epoch 641, Loss: 2.197810649871826\n",
      "Epoch 642, Loss: 2.196229934692383\n",
      "Epoch 643, Loss: 2.197697877883911\n",
      "Epoch 644, Loss: 2.1949479579925537\n",
      "Epoch 645, Loss: 2.1971681118011475\n",
      "Epoch 646, Loss: 2.1979081630706787\n",
      "Epoch 647, Loss: 2.1949265003204346\n",
      "Epoch 648, Loss: 2.1960673332214355\n",
      "Epoch 649, Loss: 2.1966304779052734\n",
      "Epoch 650, Loss: 2.1976723670959473\n",
      "Epoch 651, Loss: 2.1955084800720215\n",
      "Epoch 652, Loss: 2.1953635215759277\n",
      "Epoch 653, Loss: 2.1952247619628906\n",
      "Epoch 654, Loss: 2.1973328590393066\n",
      "Epoch 655, Loss: 2.1951217651367188\n",
      "Epoch 656, Loss: 2.1954972743988037\n",
      "Epoch 657, Loss: 2.196290969848633\n",
      "Epoch 658, Loss: 2.19529128074646\n",
      "Epoch 659, Loss: 2.195995569229126\n",
      "Epoch 660, Loss: 2.195913076400757\n",
      "Epoch 661, Loss: 2.195652484893799\n",
      "Epoch 662, Loss: 2.1945714950561523\n",
      "Epoch 663, Loss: 2.1943624019622803\n",
      "Epoch 664, Loss: 2.1953983306884766\n",
      "Epoch 665, Loss: 2.1952884197235107\n",
      "Epoch 666, Loss: 2.193575143814087\n",
      "Epoch 667, Loss: 2.194472074508667\n",
      "Epoch 668, Loss: 2.1936874389648438\n",
      "Epoch 669, Loss: 2.1933088302612305\n",
      "Epoch 670, Loss: 2.192546844482422\n",
      "Epoch 671, Loss: 2.1954333782196045\n",
      "Epoch 672, Loss: 2.192706823348999\n",
      "Epoch 673, Loss: 2.19388484954834\n",
      "Epoch 674, Loss: 2.194617748260498\n",
      "Epoch 675, Loss: 2.1921908855438232\n",
      "Epoch 676, Loss: 2.1937997341156006\n",
      "Epoch 677, Loss: 2.193150520324707\n",
      "Epoch 678, Loss: 2.193427801132202\n",
      "Epoch 679, Loss: 2.1943702697753906\n",
      "Epoch 680, Loss: 2.193542957305908\n",
      "Epoch 681, Loss: 2.1927835941314697\n",
      "Epoch 682, Loss: 2.1946427822113037\n",
      "Epoch 683, Loss: 2.1926543712615967\n",
      "Epoch 684, Loss: 2.1943161487579346\n",
      "Epoch 685, Loss: 2.192075252532959\n",
      "Epoch 686, Loss: 2.192939519882202\n",
      "Epoch 687, Loss: 2.19244647026062\n",
      "Epoch 688, Loss: 2.191396713256836\n",
      "Epoch 689, Loss: 2.192728281021118\n",
      "Epoch 690, Loss: 2.1927502155303955\n",
      "Epoch 691, Loss: 2.1925933361053467\n",
      "Epoch 692, Loss: 2.1917388439178467\n",
      "Epoch 693, Loss: 2.19156551361084\n",
      "Epoch 694, Loss: 2.192911386489868\n",
      "Epoch 695, Loss: 2.1920807361602783\n",
      "Epoch 696, Loss: 2.1916229724884033\n",
      "Epoch 697, Loss: 2.1931777000427246\n",
      "Epoch 698, Loss: 2.1893932819366455\n",
      "Epoch 699, Loss: 2.1905837059020996\n",
      "Epoch 700, Loss: 2.1908717155456543\n",
      "Epoch 701, Loss: 2.1906065940856934\n",
      "Epoch 702, Loss: 2.189333438873291\n",
      "Epoch 703, Loss: 2.190697431564331\n",
      "Epoch 704, Loss: 2.190976619720459\n",
      "Epoch 705, Loss: 2.19020414352417\n",
      "Epoch 706, Loss: 2.190822124481201\n",
      "Epoch 707, Loss: 2.1905102729797363\n",
      "Epoch 708, Loss: 2.1910111904144287\n",
      "Epoch 709, Loss: 2.190242290496826\n",
      "Epoch 710, Loss: 2.1909170150756836\n",
      "Epoch 711, Loss: 2.189100980758667\n",
      "Epoch 712, Loss: 2.1878130435943604\n",
      "Epoch 713, Loss: 2.189836263656616\n",
      "Epoch 714, Loss: 2.188549280166626\n",
      "Epoch 715, Loss: 2.1899776458740234\n",
      "Epoch 716, Loss: 2.190303325653076\n",
      "Epoch 717, Loss: 2.189481258392334\n",
      "Epoch 718, Loss: 2.18837571144104\n",
      "Epoch 719, Loss: 2.1895060539245605\n",
      "Epoch 720, Loss: 2.1906588077545166\n",
      "Epoch 721, Loss: 2.1884639263153076\n",
      "Epoch 722, Loss: 2.190753936767578\n",
      "Epoch 723, Loss: 2.1879971027374268\n",
      "Epoch 724, Loss: 2.187490224838257\n",
      "Epoch 725, Loss: 2.1886308193206787\n",
      "Epoch 726, Loss: 2.187798500061035\n",
      "Epoch 727, Loss: 2.188202381134033\n",
      "Epoch 728, Loss: 2.1892354488372803\n",
      "Epoch 729, Loss: 2.1887357234954834\n",
      "Epoch 730, Loss: 2.1872100830078125\n",
      "Epoch 731, Loss: 2.1886603832244873\n",
      "Epoch 732, Loss: 2.1873910427093506\n",
      "Epoch 733, Loss: 2.187937021255493\n",
      "Epoch 734, Loss: 2.1861486434936523\n",
      "Epoch 735, Loss: 2.1891067028045654\n",
      "Epoch 736, Loss: 2.187095880508423\n",
      "Epoch 737, Loss: 2.187647581100464\n",
      "Epoch 738, Loss: 2.186901807785034\n",
      "Epoch 739, Loss: 2.1841161251068115\n",
      "Epoch 740, Loss: 2.1874234676361084\n",
      "Epoch 741, Loss: 2.18688702583313\n",
      "Epoch 742, Loss: 2.1870737075805664\n",
      "Epoch 743, Loss: 2.1863455772399902\n",
      "Epoch 744, Loss: 2.185880661010742\n",
      "Epoch 745, Loss: 2.185981035232544\n",
      "Epoch 746, Loss: 2.186771869659424\n",
      "Epoch 747, Loss: 2.1856637001037598\n",
      "Epoch 748, Loss: 2.1847522258758545\n",
      "Epoch 749, Loss: 2.1856908798217773\n",
      "Epoch 750, Loss: 2.185110569000244\n",
      "Epoch 751, Loss: 2.185624837875366\n",
      "Epoch 752, Loss: 2.185249090194702\n",
      "Epoch 753, Loss: 2.184473991394043\n",
      "Epoch 754, Loss: 2.1860313415527344\n",
      "Epoch 755, Loss: 2.185420513153076\n",
      "Epoch 756, Loss: 2.1850712299346924\n",
      "Epoch 757, Loss: 2.1852104663848877\n",
      "Epoch 758, Loss: 2.1856911182403564\n",
      "Epoch 759, Loss: 2.1845686435699463\n",
      "Epoch 760, Loss: 2.1845529079437256\n",
      "Epoch 761, Loss: 2.1842093467712402\n",
      "Epoch 762, Loss: 2.1856980323791504\n",
      "Epoch 763, Loss: 2.181979179382324\n",
      "Epoch 764, Loss: 2.1847338676452637\n",
      "Epoch 765, Loss: 2.184974431991577\n",
      "Epoch 766, Loss: 2.184804677963257\n",
      "Epoch 767, Loss: 2.183812141418457\n",
      "Epoch 768, Loss: 2.1835484504699707\n",
      "Epoch 769, Loss: 2.1835033893585205\n",
      "Epoch 770, Loss: 2.1825218200683594\n",
      "Epoch 771, Loss: 2.1859347820281982\n",
      "Epoch 772, Loss: 2.185507297515869\n",
      "Epoch 773, Loss: 2.184260606765747\n",
      "Epoch 774, Loss: 2.1838722229003906\n",
      "Epoch 775, Loss: 2.1825740337371826\n",
      "Epoch 776, Loss: 2.1819186210632324\n",
      "Epoch 777, Loss: 2.184812545776367\n",
      "Epoch 778, Loss: 2.1833655834198\n",
      "Epoch 779, Loss: 2.1839301586151123\n",
      "Epoch 780, Loss: 2.1827499866485596\n",
      "Epoch 781, Loss: 2.182788133621216\n",
      "Epoch 782, Loss: 2.1829237937927246\n",
      "Epoch 783, Loss: 2.181955099105835\n",
      "Epoch 784, Loss: 2.1819558143615723\n",
      "Epoch 785, Loss: 2.183248996734619\n",
      "Epoch 786, Loss: 2.1821482181549072\n",
      "Epoch 787, Loss: 2.1819279193878174\n",
      "Epoch 788, Loss: 2.182342290878296\n",
      "Epoch 789, Loss: 2.1825103759765625\n",
      "Epoch 790, Loss: 2.1822099685668945\n",
      "Epoch 791, Loss: 2.18245005607605\n",
      "Epoch 792, Loss: 2.181826114654541\n",
      "Epoch 793, Loss: 2.1815662384033203\n",
      "Epoch 794, Loss: 2.1817007064819336\n",
      "Epoch 795, Loss: 2.182407855987549\n",
      "Epoch 796, Loss: 2.182905435562134\n",
      "Epoch 797, Loss: 2.1801085472106934\n",
      "Epoch 798, Loss: 2.1814568042755127\n",
      "Epoch 799, Loss: 2.180865526199341\n",
      "Epoch 800, Loss: 2.1812191009521484\n",
      "Epoch 801, Loss: 2.1792490482330322\n",
      "Epoch 802, Loss: 2.180647134780884\n",
      "Epoch 803, Loss: 2.1803057193756104\n",
      "Epoch 804, Loss: 2.1804890632629395\n",
      "Epoch 805, Loss: 2.18096661567688\n",
      "Epoch 806, Loss: 2.18043851852417\n",
      "Epoch 807, Loss: 2.179659843444824\n",
      "Epoch 808, Loss: 2.183053493499756\n",
      "Epoch 809, Loss: 2.1802175045013428\n",
      "Epoch 810, Loss: 2.1805503368377686\n",
      "Epoch 811, Loss: 2.181519031524658\n",
      "Epoch 812, Loss: 2.181818723678589\n",
      "Epoch 813, Loss: 2.181487560272217\n",
      "Epoch 814, Loss: 2.1800625324249268\n",
      "Epoch 815, Loss: 2.1802432537078857\n",
      "Epoch 816, Loss: 2.1805126667022705\n",
      "Epoch 817, Loss: 2.182149887084961\n",
      "Epoch 818, Loss: 2.1794657707214355\n",
      "Epoch 819, Loss: 2.1799299716949463\n",
      "Epoch 820, Loss: 2.1792874336242676\n",
      "Epoch 821, Loss: 2.179412603378296\n",
      "Epoch 822, Loss: 2.1817381381988525\n",
      "Epoch 823, Loss: 2.1801609992980957\n",
      "Epoch 824, Loss: 2.1785552501678467\n",
      "Epoch 825, Loss: 2.1808440685272217\n",
      "Epoch 826, Loss: 2.1795551776885986\n",
      "Epoch 827, Loss: 2.179025650024414\n",
      "Epoch 828, Loss: 2.1799089908599854\n",
      "Epoch 829, Loss: 2.1797986030578613\n",
      "Epoch 830, Loss: 2.1781082153320312\n",
      "Epoch 831, Loss: 2.1819047927856445\n",
      "Epoch 832, Loss: 2.1804776191711426\n",
      "Epoch 833, Loss: 2.178480625152588\n",
      "Epoch 834, Loss: 2.178600788116455\n",
      "Epoch 835, Loss: 2.179311752319336\n",
      "Epoch 836, Loss: 2.1798079013824463\n",
      "Epoch 837, Loss: 2.1772680282592773\n",
      "Epoch 838, Loss: 2.179898262023926\n",
      "Epoch 839, Loss: 2.179041862487793\n",
      "Epoch 840, Loss: 2.179717540740967\n",
      "Epoch 841, Loss: 2.1778175830841064\n",
      "Epoch 842, Loss: 2.180854320526123\n",
      "Epoch 843, Loss: 2.181241273880005\n",
      "Epoch 844, Loss: 2.1794064044952393\n",
      "Epoch 845, Loss: 2.1786835193634033\n",
      "Epoch 846, Loss: 2.1767184734344482\n",
      "Epoch 847, Loss: 2.179363489151001\n",
      "Epoch 848, Loss: 2.17895245552063\n",
      "Epoch 849, Loss: 2.1790645122528076\n",
      "Epoch 850, Loss: 2.1765592098236084\n",
      "Epoch 851, Loss: 2.178457021713257\n",
      "Epoch 852, Loss: 2.1779143810272217\n",
      "Epoch 853, Loss: 2.1781280040740967\n",
      "Epoch 854, Loss: 2.1783864498138428\n",
      "Epoch 855, Loss: 2.1793107986450195\n",
      "Epoch 856, Loss: 2.1784651279449463\n",
      "Epoch 857, Loss: 2.176724672317505\n",
      "Epoch 858, Loss: 2.1770246028900146\n",
      "Epoch 859, Loss: 2.1795434951782227\n",
      "Epoch 860, Loss: 2.177314519882202\n",
      "Epoch 861, Loss: 2.179250955581665\n",
      "Epoch 862, Loss: 2.1780660152435303\n",
      "Epoch 863, Loss: 2.178213596343994\n",
      "Epoch 864, Loss: 2.1787006855010986\n",
      "Epoch 865, Loss: 2.176880359649658\n",
      "Epoch 866, Loss: 2.1792659759521484\n",
      "Epoch 867, Loss: 2.177441358566284\n",
      "Epoch 868, Loss: 2.176313877105713\n",
      "Epoch 869, Loss: 2.1775975227355957\n",
      "Epoch 870, Loss: 2.175757884979248\n",
      "Epoch 871, Loss: 2.175157070159912\n",
      "Epoch 872, Loss: 2.177067518234253\n",
      "Epoch 873, Loss: 2.175999402999878\n",
      "Epoch 874, Loss: 2.177622079849243\n",
      "Epoch 875, Loss: 2.176865577697754\n",
      "Epoch 876, Loss: 2.1763691902160645\n",
      "Epoch 877, Loss: 2.176345109939575\n",
      "Epoch 878, Loss: 2.1763153076171875\n",
      "Epoch 879, Loss: 2.176349639892578\n",
      "Epoch 880, Loss: 2.177086591720581\n",
      "Epoch 881, Loss: 2.1788015365600586\n",
      "Epoch 882, Loss: 2.1763761043548584\n",
      "Epoch 883, Loss: 2.17677640914917\n",
      "Epoch 884, Loss: 2.177034378051758\n",
      "Epoch 885, Loss: 2.1767895221710205\n",
      "Epoch 886, Loss: 2.177065134048462\n",
      "Epoch 887, Loss: 2.1781399250030518\n",
      "Epoch 888, Loss: 2.1767430305480957\n",
      "Epoch 889, Loss: 2.176539182662964\n",
      "Epoch 890, Loss: 2.1774446964263916\n",
      "Epoch 891, Loss: 2.1756439208984375\n",
      "Epoch 892, Loss: 2.1765735149383545\n",
      "Epoch 893, Loss: 2.1760873794555664\n",
      "Epoch 894, Loss: 2.1766419410705566\n",
      "Epoch 895, Loss: 2.1767561435699463\n",
      "Epoch 896, Loss: 2.175320625305176\n",
      "Epoch 897, Loss: 2.1790645122528076\n",
      "Epoch 898, Loss: 2.177323579788208\n",
      "Epoch 899, Loss: 2.174222707748413\n",
      "Epoch 900, Loss: 2.1768836975097656\n",
      "Epoch 901, Loss: 2.1779263019561768\n",
      "Epoch 902, Loss: 2.176360845565796\n",
      "Epoch 903, Loss: 2.176098585128784\n",
      "Epoch 904, Loss: 2.1742923259735107\n",
      "Epoch 905, Loss: 2.1766128540039062\n",
      "Epoch 906, Loss: 2.176013946533203\n",
      "Epoch 907, Loss: 2.176225423812866\n",
      "Epoch 908, Loss: 2.175870656967163\n",
      "Epoch 909, Loss: 2.176056146621704\n",
      "Epoch 910, Loss: 2.177250862121582\n",
      "Epoch 911, Loss: 2.174978256225586\n",
      "Epoch 912, Loss: 2.1752593517303467\n",
      "Epoch 913, Loss: 2.174191474914551\n",
      "Epoch 914, Loss: 2.1755881309509277\n",
      "Epoch 915, Loss: 2.174875497817993\n",
      "Epoch 916, Loss: 2.1771886348724365\n",
      "Epoch 917, Loss: 2.175604820251465\n",
      "Epoch 918, Loss: 2.175583600997925\n",
      "Epoch 919, Loss: 2.175065279006958\n",
      "Epoch 920, Loss: 2.17431378364563\n",
      "Epoch 921, Loss: 2.1767706871032715\n",
      "Epoch 922, Loss: 2.174586296081543\n",
      "Epoch 923, Loss: 2.175114870071411\n",
      "Epoch 924, Loss: 2.1756296157836914\n",
      "Epoch 925, Loss: 2.174682855606079\n",
      "Epoch 926, Loss: 2.1747686862945557\n",
      "Epoch 927, Loss: 2.1753718852996826\n",
      "Epoch 928, Loss: 2.1761674880981445\n",
      "Epoch 929, Loss: 2.1732571125030518\n",
      "Epoch 930, Loss: 2.176535129547119\n",
      "Epoch 931, Loss: 2.1767220497131348\n",
      "Epoch 932, Loss: 2.1765384674072266\n",
      "Epoch 933, Loss: 2.175945281982422\n",
      "Epoch 934, Loss: 2.1767892837524414\n",
      "Epoch 935, Loss: 2.175157070159912\n",
      "Epoch 936, Loss: 2.1766679286956787\n",
      "Epoch 937, Loss: 2.1755189895629883\n",
      "Epoch 938, Loss: 2.177457332611084\n",
      "Epoch 939, Loss: 2.176459550857544\n",
      "Epoch 940, Loss: 2.1768202781677246\n",
      "Epoch 941, Loss: 2.1736481189727783\n",
      "Epoch 942, Loss: 2.1746158599853516\n",
      "Epoch 943, Loss: 2.174247980117798\n",
      "Epoch 944, Loss: 2.177473783493042\n",
      "Epoch 945, Loss: 2.1740033626556396\n",
      "Epoch 946, Loss: 2.1751012802124023\n",
      "Epoch 947, Loss: 2.176011323928833\n",
      "Epoch 948, Loss: 2.1750667095184326\n",
      "Epoch 949, Loss: 2.1739208698272705\n",
      "Epoch 950, Loss: 2.1757476329803467\n",
      "Epoch 951, Loss: 2.175283432006836\n",
      "Epoch 952, Loss: 2.1729841232299805\n",
      "Epoch 953, Loss: 2.175565242767334\n",
      "Epoch 954, Loss: 2.17353892326355\n",
      "Epoch 955, Loss: 2.174786329269409\n",
      "Epoch 956, Loss: 2.175633430480957\n",
      "Epoch 957, Loss: 2.175992727279663\n",
      "Epoch 958, Loss: 2.174955368041992\n",
      "Epoch 959, Loss: 2.176459312438965\n",
      "Epoch 960, Loss: 2.1755733489990234\n",
      "Epoch 961, Loss: 2.176192045211792\n",
      "Epoch 962, Loss: 2.173780679702759\n",
      "Epoch 963, Loss: 2.1744883060455322\n",
      "Epoch 964, Loss: 2.1768226623535156\n",
      "Epoch 965, Loss: 2.175182819366455\n",
      "Epoch 966, Loss: 2.175603151321411\n",
      "Epoch 967, Loss: 2.174168825149536\n",
      "Epoch 968, Loss: 2.1744871139526367\n",
      "Epoch 969, Loss: 2.174639940261841\n",
      "Epoch 970, Loss: 2.1751060485839844\n",
      "Epoch 971, Loss: 2.175487518310547\n",
      "Epoch 972, Loss: 2.175159215927124\n",
      "Epoch 973, Loss: 2.174180746078491\n",
      "Epoch 974, Loss: 2.1751902103424072\n",
      "Epoch 975, Loss: 2.1756997108459473\n",
      "Epoch 976, Loss: 2.17549729347229\n",
      "Epoch 977, Loss: 2.1735706329345703\n",
      "Epoch 978, Loss: 2.173006534576416\n",
      "Epoch 979, Loss: 2.173558473587036\n",
      "Epoch 980, Loss: 2.1731040477752686\n",
      "Epoch 981, Loss: 2.174353837966919\n",
      "Epoch 982, Loss: 2.174805164337158\n",
      "Epoch 983, Loss: 2.174739122390747\n",
      "Epoch 984, Loss: 2.174715995788574\n",
      "Epoch 985, Loss: 2.175410032272339\n",
      "Epoch 986, Loss: 2.1745150089263916\n",
      "Epoch 987, Loss: 2.1750476360321045\n",
      "Epoch 988, Loss: 2.1739914417266846\n",
      "Epoch 989, Loss: 2.1774075031280518\n",
      "Epoch 990, Loss: 2.174704074859619\n",
      "Epoch 991, Loss: 2.173363447189331\n",
      "Epoch 992, Loss: 2.175067663192749\n",
      "Epoch 993, Loss: 2.1746597290039062\n",
      "Epoch 994, Loss: 2.1739795207977295\n",
      "Epoch 995, Loss: 2.1740903854370117\n",
      "Epoch 996, Loss: 2.175124406814575\n",
      "Epoch 997, Loss: 2.1745548248291016\n",
      "Epoch 998, Loss: 2.174227476119995\n",
      "Epoch 999, Loss: 2.174436569213867\n",
      "Epoch 1000, Loss: 2.1739373207092285\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Definir un clasificador con Dropout\n",
    "class NodeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Definir el número de clases\n",
    "num_classes = len(torch.unique(labels))\n",
    "\n",
    "# Obtener las representaciones de los nodos\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    node_embeddings = model.encode(data.x, data.edge_index)\n",
    "\n",
    "# Inicializar el clasificador\n",
    "classifier = NodeClassifier(input_dim=node_embeddings.size(1), hidden_dim=64, output_dim=num_classes)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01, weight_decay=5e-4)  # Añadir L2 regularización\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Bucle de entrenamiento para el clasificador\n",
    "for epoch in range(1000):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = classifier(node_embeddings)\n",
    "    loss = criterion(out, labels)  # 'labels' debe ser un tensor con las etiquetas de los nodos\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.24808790261768826\n",
      "Precision: 0.13274945669972377\n",
      "Recall: 0.24808790261768826\n",
      "F1 Score: 0.13499726957332053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Conda\\envs\\spotify-network\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Evaluar el clasificador\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    pred = classifier(node_embeddings).argmax(dim=1)\n",
    "    accuracy = accuracy_score(labels.cpu(), pred.cpu())\n",
    "    precision = precision_score(labels.cpu(), pred.cpu(), average='weighted')\n",
    "    recall = recall_score(labels.cpu(), pred.cpu(), average='weighted')\n",
    "    f1 = f1_score(labels.cpu(), pred.cpu(), average='weighted')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Conda\\envs\\spotify-network\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Apps\\Conda\\envs\\spotify-network\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Apps\\Conda\\envs\\spotify-network\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Apps\\Conda\\envs\\spotify-network\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Apps\\Conda\\envs\\spotify-network\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Apps\\Conda\\envs\\spotify-network\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Apps\\Conda\\envs\\spotify-network\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Apps\\Conda\\envs\\spotify-network\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Apps\\Conda\\envs\\spotify-network\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.3639976012397461\n",
      "Average Precision: 0.311928957251956\n",
      "Average Recall: 0.36399760123974606\n",
      "Average F1 Score: 0.30986871794342086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Conda\\envs\\spotify-network\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Suponiendo que 'node_embeddings' y 'labels' ya están definidos\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(node_embeddings):\n",
    "    X_train, X_test = node_embeddings[train_index], node_embeddings[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    classifier = NodeClassifier(input_dim=node_embeddings.size(1), hidden_dim=100, output_dim=num_classes)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Entrenamiento del clasificador\n",
    "    for epoch in range(1000):\n",
    "        classifier.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = classifier(X_train)\n",
    "        loss = criterion(out, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluación del clasificador\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = classifier(X_test).argmax(dim=1)\n",
    "        accuracies.append(accuracy_score(y_test.cpu(), pred.cpu()))\n",
    "        precisions.append(precision_score(y_test.cpu(), pred.cpu(), average='weighted'))\n",
    "        recalls.append(recall_score(y_test.cpu(), pred.cpu(), average='weighted'))\n",
    "        f1_scores.append(f1_score(y_test.cpu(), pred.cpu(), average='weighted'))\n",
    "\n",
    "print(f'Average Accuracy: {sum(accuracies) / len(accuracies)}')\n",
    "print(f'Average Precision: {sum(precisions) / len(precisions)}')\n",
    "print(f'Average Recall: {sum(recalls) / len(recalls)}')\n",
    "print(f'Average F1 Score: {sum(f1_scores) / len(f1_scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Definir un clasificador mejorado con más capas, Batch Normalization y Dropout\n",
    "class ImprovedNodeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(ImprovedNodeClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
    "        self.dropout2 = nn.Dropout(p=0.25)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Ejemplo de uso\n",
    "input_dim = node_embeddings.size(1)\n",
    "hidden_dim1 = 64\n",
    "hidden_dim2 = 32\n",
    "output_dim = num_classes\n",
    "\n",
    "model = ImprovedNodeClassifier(input_dim, hidden_dim1, hidden_dim2, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.3107614517211914\n",
      "Epoch 2, Loss: 3.09382963180542\n",
      "Epoch 3, Loss: 2.959340810775757\n",
      "Epoch 4, Loss: 2.8455047607421875\n",
      "Epoch 5, Loss: 2.7401890754699707\n",
      "Epoch 6, Loss: 2.6299726963043213\n",
      "Epoch 7, Loss: 2.548335075378418\n",
      "Epoch 8, Loss: 2.4782910346984863\n",
      "Epoch 9, Loss: 2.4219117164611816\n",
      "Epoch 10, Loss: 2.3759562969207764\n",
      "Epoch 11, Loss: 2.327646017074585\n",
      "Epoch 12, Loss: 2.2884724140167236\n",
      "Epoch 13, Loss: 2.2627952098846436\n",
      "Epoch 14, Loss: 2.2387607097625732\n",
      "Epoch 15, Loss: 2.2145142555236816\n",
      "Epoch 16, Loss: 2.1930041313171387\n",
      "Epoch 17, Loss: 2.1732118129730225\n",
      "Epoch 18, Loss: 2.1656274795532227\n",
      "Epoch 19, Loss: 2.1509721279144287\n",
      "Epoch 20, Loss: 2.133121967315674\n",
      "Epoch 21, Loss: 2.1289424896240234\n",
      "Epoch 22, Loss: 2.115555763244629\n",
      "Epoch 23, Loss: 2.1133201122283936\n",
      "Epoch 24, Loss: 2.094125270843506\n",
      "Epoch 25, Loss: 2.0840322971343994\n",
      "Epoch 26, Loss: 2.075995445251465\n",
      "Epoch 27, Loss: 2.0717360973358154\n",
      "Epoch 28, Loss: 2.068911552429199\n",
      "Epoch 29, Loss: 2.0534636974334717\n",
      "Epoch 30, Loss: 2.0505762100219727\n",
      "Epoch 31, Loss: 2.051093339920044\n",
      "Epoch 32, Loss: 2.0400469303131104\n",
      "Epoch 33, Loss: 2.04229998588562\n",
      "Epoch 34, Loss: 2.026812791824341\n",
      "Epoch 35, Loss: 2.020949363708496\n",
      "Epoch 36, Loss: 2.0181541442871094\n",
      "Epoch 37, Loss: 2.0238866806030273\n",
      "Epoch 38, Loss: 2.006169557571411\n",
      "Epoch 39, Loss: 1.9972435235977173\n",
      "Epoch 40, Loss: 1.9886269569396973\n",
      "Epoch 41, Loss: 1.9905507564544678\n",
      "Epoch 42, Loss: 1.9774867296218872\n",
      "Epoch 43, Loss: 1.9727153778076172\n",
      "Epoch 44, Loss: 1.9740840196609497\n",
      "Epoch 45, Loss: 1.9656710624694824\n",
      "Epoch 46, Loss: 1.9529387950897217\n",
      "Epoch 47, Loss: 1.966639518737793\n",
      "Epoch 48, Loss: 1.9781432151794434\n",
      "Epoch 49, Loss: 1.966012716293335\n",
      "Epoch 50, Loss: 1.9491392374038696\n",
      "Epoch 51, Loss: 1.9523078203201294\n",
      "Epoch 52, Loss: 1.9320292472839355\n",
      "Epoch 53, Loss: 1.9331260919570923\n",
      "Epoch 54, Loss: 1.937718391418457\n",
      "Epoch 55, Loss: 1.9283769130706787\n",
      "Epoch 56, Loss: 1.9234908819198608\n",
      "Epoch 57, Loss: 1.9249054193496704\n",
      "Epoch 58, Loss: 1.918737530708313\n",
      "Epoch 59, Loss: 1.913672685623169\n",
      "Epoch 60, Loss: 1.9176266193389893\n",
      "Epoch 61, Loss: 1.9222019910812378\n",
      "Epoch 62, Loss: 1.917765736579895\n",
      "Epoch 63, Loss: 1.9051984548568726\n",
      "Epoch 64, Loss: 1.8999847173690796\n",
      "Epoch 65, Loss: 1.9089115858078003\n",
      "Epoch 66, Loss: 1.9075018167495728\n",
      "Epoch 67, Loss: 1.8991907835006714\n",
      "Epoch 68, Loss: 1.8887816667556763\n",
      "Epoch 69, Loss: 1.8989847898483276\n",
      "Epoch 70, Loss: 1.9010565280914307\n",
      "Epoch 71, Loss: 1.9007424116134644\n",
      "Epoch 72, Loss: 1.8898111581802368\n",
      "Epoch 73, Loss: 1.8988391160964966\n",
      "Epoch 74, Loss: 1.905285358428955\n",
      "Epoch 75, Loss: 1.8907356262207031\n",
      "Epoch 76, Loss: 1.8856593370437622\n",
      "Epoch 77, Loss: 1.8936219215393066\n",
      "Epoch 78, Loss: 1.8828710317611694\n",
      "Epoch 79, Loss: 1.885171890258789\n",
      "Epoch 80, Loss: 1.8823473453521729\n",
      "Epoch 81, Loss: 1.8778002262115479\n",
      "Epoch 82, Loss: 1.8783758878707886\n",
      "Epoch 83, Loss: 1.8821964263916016\n",
      "Epoch 84, Loss: 1.8708913326263428\n",
      "Epoch 85, Loss: 1.8760366439819336\n",
      "Epoch 86, Loss: 1.8636982440948486\n",
      "Epoch 87, Loss: 1.8530430793762207\n",
      "Epoch 88, Loss: 1.8579556941986084\n",
      "Epoch 89, Loss: 1.8816715478897095\n",
      "Epoch 90, Loss: 1.8568342924118042\n",
      "Epoch 91, Loss: 1.8500045537948608\n",
      "Epoch 92, Loss: 1.8752362728118896\n",
      "Epoch 93, Loss: 1.8591632843017578\n",
      "Epoch 94, Loss: 1.8606358766555786\n",
      "Epoch 95, Loss: 1.8513561487197876\n",
      "Epoch 96, Loss: 1.8486087322235107\n",
      "Epoch 97, Loss: 1.862717628479004\n",
      "Epoch 98, Loss: 1.8689491748809814\n",
      "Epoch 99, Loss: 1.8681097030639648\n",
      "Epoch 100, Loss: 1.8509796857833862\n",
      "Accuracy: 0.40159431218356134\n",
      "Precision: 0.40084985178200233\n",
      "Recall: 0.40159431218356134\n",
      "F1 Score: 0.35426894549468785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Conda\\envs\\spotify-network\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Inicializar el clasificador mejorado\n",
    "classifier = ImprovedNodeClassifier(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Bucle de entrenamiento para el clasificador mejorado\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = classifier(node_embeddings)\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Evaluar el clasificador mejorado\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    pred = classifier(node_embeddings).argmax(dim=1)\n",
    "    accuracy = accuracy_score(labels.cpu(), pred.cpu())\n",
    "    precision = precision_score(labels.cpu(), pred.cpu(), average='weighted')\n",
    "    recall = recall_score(labels.cpu(), pred.cpu(), average='weighted')\n",
    "    f1 = f1_score(labels.cpu(), pred.cpu(), average='weighted')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spotify-network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
